{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43adb73-36b3-481f-954e-1f83ca69949f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast, TrainingArguments, Trainer, BertForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b89fa6f-53b5-4c7c-bfb1-ff894695c676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cf2a4b-8ae3-4444-9e65-1f9cc9d2d003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "BATCH_SIZE = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babaae92-f01d-45d7-8f44-f6081f76a235",
   "metadata": {},
   "source": [
    "### Choosing the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e370eb4e-10ef-49e3-b6cc-f81395f591b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1, ignore_mismatched_sizes=True)\n",
    "bert_model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f851fd-2fbe-4590-acda-110933fdc526",
   "metadata": {},
   "source": [
    "### Reading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e2dc8c-5490-4ce2-9c6d-366085c52c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>Illustrious Sir: I have the honor to hand to your Royal Highness the letter by which his Majesty the Emperor of Russia has deigned to accredit me by his Majesty the King of Serbia.\\nMy august master has charged me to express to you the vivid sympathy and the sincere admiration which his Majesty ...</td>\n",
       "      <td>-2.634137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>By looking at any map of Europe, it will be seen that England is separated from France by the English Channel, a passage which, though it looks quite narrow on the map, is really very wide, especially toward the west. The narrowest place is between Dover and Calais, where the distance across is ...</td>\n",
       "      <td>-1.188881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>When the ambassadors had returned to Rome the Senate commanded that there should be levied two armies; and that Minucius the Consul should march with the one against the Æquians on Mount Ægidus, and that the other should hinder the enemy from their plundering. This levying the tribunes of the Co...</td>\n",
       "      <td>-3.218972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>Having just made the trip from Salt Lake City to this place on the Denver &amp; Rio Grande line, I cannot write you on any other subject at present. There is not in the world a railroad journey of thirty hours so filled with grand and beautiful views. I should perhaps qualify this statement by deduc...</td>\n",
       "      <td>-1.215504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>It was Saturday evening; the sun was setting, the workpeople were coming in crowds from the factory to the station, and they bowed to the carriage in which Korolyov was driving. And he was charmed with the evening, the farmhouses and villas on the road, and the birch-trees, and the quiet atmosph...</td>\n",
       "      <td>-1.212478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             text   \n",
       "1367  Illustrious Sir: I have the honor to hand to your Royal Highness the letter by which his Majesty the Emperor of Russia has deigned to accredit me by his Majesty the King of Serbia.\\nMy august master has charged me to express to you the vivid sympathy and the sincere admiration which his Majesty ...  \\\n",
       "2399  By looking at any map of Europe, it will be seen that England is separated from France by the English Channel, a passage which, though it looks quite narrow on the map, is really very wide, especially toward the west. The narrowest place is between Dover and Calais, where the distance across is ...   \n",
       "2616  When the ambassadors had returned to Rome the Senate commanded that there should be levied two armies; and that Minucius the Consul should march with the one against the Æquians on Mount Ægidus, and that the other should hinder the enemy from their plundering. This levying the tribunes of the Co...   \n",
       "1671  Having just made the trip from Salt Lake City to this place on the Denver & Rio Grande line, I cannot write you on any other subject at present. There is not in the world a railroad journey of thirty hours so filled with grand and beautiful views. I should perhaps qualify this statement by deduc...   \n",
       "1497  It was Saturday evening; the sun was setting, the workpeople were coming in crowds from the factory to the station, and they bowed to the carriage in which Korolyov was driving. And he was charmed with the evening, the farmhouses and villas on the road, and the birch-trees, and the quiet atmosph...   \n",
       "\n",
       "        labels  \n",
       "1367 -2.634137  \n",
       "2399 -1.188881  \n",
       "2616 -3.218972  \n",
       "1671 -1.215504  \n",
       "1497 -1.212478  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../../../data/input/train.csv.zip\", usecols=[3, 4])\n",
    "train_df = train_df.rename(columns={'excerpt': 'text', 'target': 'labels'})\n",
    "train_df, eval_df = train_test_split(train_df, test_size=0.1, stratify=pd.cut(train_df[\"labels\"], 5), random_state=SEED)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78575c60-617a-4ffc-9376-7018a23cebd0",
   "metadata": {},
   "source": [
    "### Converting the dataset into transformer-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c04a10-6e62-471c-8d05-3d26f133df21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(train, evaluation):\n",
    "    dataset = DatasetDict()\n",
    "    dataset['train'] = Dataset.from_dict(train.to_dict(orient='list'), split=\"train\")\n",
    "    dataset['eval'] = Dataset.from_dict(evaluation.to_dict(orient='list'), split=\"eval\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = create_dataset(train_df, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362b448-9f08-41ef-a9a6-01c000089ae3",
   "metadata": {},
   "source": [
    "### Preparing the dataset for feeding into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960551ca-7454-4183-8cce-b63cc18f0cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=300, return_tensors=\"pt\", return_attention_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, batch_size=64, remove_columns=\"text\")\n",
    "tokenized_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c31008-f7a7-4072-a2b6-d5b96d8d2a8e",
   "metadata": {},
   "source": [
    "### Setting all training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87da54f-5ab6-4a8c-a4d9-b9dcdd25fcc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3.0,\n",
    "    learning_rate=1e-5,\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b19ac-d3a9-4a02-bb21-c8750ad57f17",
   "metadata": {},
   "source": [
    "### Calculating RMSE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f4a28b8-a014-43fa-a20b-eccfcbcbcf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = torch.from_numpy(pred.label_ids)\n",
    "    preds = torch.from_numpy(pred.predictions).squeeze()\n",
    "    mse = torch.mean((preds - labels) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d412613-0f57-4859-a525-8463c5e6ff32",
   "metadata": {},
   "source": [
    "### Creating custom trainer\n",
    "We need it for calculating RMSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f984ea-4e93-4c09-97b4-1443b2d61517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertRegressorTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(**inputs)\n",
    "        loss = torch.sqrt(nn.functional.mse_loss(outputs[\"logits\"].squeeze(), labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db75357-4c99-41ae-a185-2a7dcc3b1e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = BertRegressorTrainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'].shuffle(seed=SEED),\n",
    "    eval_dataset=tokenized_dataset['eval'].shuffle(seed=SEED),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "587f9dac-3556-43e5-8991-224a17dc3c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2550\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 30\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 30\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 425\n",
      "  Number of trainable parameters = 108311041\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='425' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [425/425 06:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.687693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.609670</td>\n",
       "      <td>0.614483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.547744</td>\n",
       "      <td>0.551593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590452</td>\n",
       "      <td>0.594233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.610101</td>\n",
       "      <td>0.613585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 284\n",
      "  Batch size = 30\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 284\n",
      "  Batch size = 30\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 284\n",
      "  Batch size = 30\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 284\n",
      "  Batch size = 30\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 284\n",
      "  Batch size = 30\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=425, training_loss=0.5953230195886948, metrics={'train_runtime': 386.4163, 'train_samples_per_second': 32.996, 'train_steps_per_second': 1.1, 'total_flos': 1965606934950000.0, 'train_loss': 0.5953230195886948, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a292c2-c688-47dd-9905-8648b7f0a4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc3732ac-4a35-4455-8043-2fbff4df7738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1feda81-7341-4500-bd5f-6f7614ffa5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
